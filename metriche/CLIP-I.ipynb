{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ltp1uLrCJ7zO"},"outputs":[],"source":["#@title Installazione librerie\n","!pip install transformers diffusers torchvision"]},{"cell_type":"code","source":["#@title Upload foto riferimenti\n","\n","from google.colab import files\n","import os\n","\n","def save_uploaded_files(save_directory):\n","    uploaded = files.upload()  # apre il selettore per l'upload\n","\n","    os.makedirs(save_directory, exist_ok=True)\n","\n","    for filename, content in uploaded.items():\n","        filepath = os.path.join(save_directory, filename)\n","        with open(filepath, 'wb') as f:\n","            f.write(content)\n","        print(f\"âœ… Salvato: {filepath}\")"],"metadata":{"id":"KmlCxQmAHeUv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@markdown Inserisci il percorso in cui salvare i file (verrÃ  creato se non esiste):\n","\n","save_path = \"./dataset/andrea/Dreambooth2\" #@param {type:\"string\"}\n","save_uploaded_files(save_path)"],"metadata":{"id":"PvmES4pGH7r7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Upload file rar ed estrazione\n","\n","!pip install rarfile unrar\n","\n","import rarfile\n","\n","def upload_and_extract_rar(save_directory):\n","    uploaded = files.upload()  # carica file RAR\n","\n","    os.makedirs(save_directory, exist_ok=True)\n","\n","    for filename in uploaded.keys():\n","        rar_path = os.path.join(save_directory, filename)\n","        with open(rar_path, 'wb') as f:\n","            f.write(uploaded[filename])\n","\n","        print(f\"âœ… RAR salvato: {rar_path}\")\n","\n","        # estrai con rarfile\n","        with rarfile.RarFile(rar_path) as rf:\n","            rf.extractall(save_directory)\n","            print(f\"ðŸ“‚ Estratto in: {save_directory}\")\n","\n","        os.remove(rar_path)\n"],"metadata":{"id":"3XhQGgeLOKmg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@markdown Inserisci il percorso in cui salvare i file (verrÃ  creato se non esiste):\n","\n","save_path = \"./dataset/test/\" #@param {type:\"string\"}\n","upload_and_extract_rar(save_path)"],"metadata":{"id":"uWmf3ABEIVh9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Funzione per computare il CLIP-I di un img su un set di riferimenti e calcolarne il valore medio\n","\n","import os\n","from transformers import CLIPProcessor, CLIPModel\n","from PIL import Image\n","import torch\n","\n","#Calcola la CLIP similarity tra un'immagine generata e un set di immagini di riferimento.\n","def compute_clip_similarity(ref_dir, gen_image_path, device=\"cpu\"):\n","\n","    # Carica il modello CLIP\n","    clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n","    clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n","\n","    # Trova immagini di riferimento\n","    valid_exts = [\".jpg\", \".jpeg\", \".png\"]\n","    ref_image_paths = [\n","        os.path.join(ref_dir, fname)\n","        for fname in os.listdir(ref_dir)\n","        if os.path.splitext(fname)[1].lower() in valid_exts\n","    ]\n","\n","    if len(ref_image_paths) == 0:\n","        raise ValueError(f\"Nessuna immagine valida trovata in {ref_dir}\")\n","\n","    # Carica immagini di riferimento\n","    ref_images = [Image.open(path).convert(\"RGB\") for path in ref_image_paths]\n","\n","    # Prepara immagini di riferimento\n","    inputs_ref = clip_processor(images=ref_images, return_tensors=\"pt\", padding=True).to(device)\n","\n","    # Estrai embeddings immagini di riferimento\n","    with torch.no_grad():\n","        ref_features = clip_model.get_image_features(**inputs_ref)\n","\n","    # Normalizza embeddings\n","    ref_features = ref_features / ref_features.norm(p=2, dim=-1, keepdim=True)\n","\n","    # Carica immagine generata\n","    image_gen = Image.open(gen_image_path).convert(\"RGB\")\n","\n","    # Prepara immagine generata\n","    inputs_gen = clip_processor(images=[image_gen], return_tensors=\"pt\").to(device)\n","\n","    # Estrai embedding immagine generata\n","    with torch.no_grad():\n","        gen_feature = clip_model.get_image_features(**inputs_gen)\n","\n","    # Normalizza embedding immagine generata\n","    gen_feature = gen_feature / gen_feature.norm(p=2, dim=-1, keepdim=True)\n","\n","    # Calcola cosine similarity per ogni immagine di riferimento\n","    similarities = []\n","    for ref_feat in ref_features:\n","        sim = torch.nn.functional.cosine_similarity(gen_feature[0], ref_feat, dim=0)\n","        similarities.append(sim.item())\n","\n","    # Calcola media delle similaritÃ \n","    mean_similarity = sum(similarities) / len(similarities)\n","\n","    # Output\n","    result = {\n","        \"ref_image_paths\": ref_image_paths,\n","        \"similarities\": similarities,\n","        \"mean_similarity\": mean_similarity\n","    }\n","\n","    return result"],"metadata":{"id":"Bn5NNh1LKDIt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Computazione inserendo i percorsi che ci servono\n","\n","result = compute_clip_similarity(\"/content/dataset/cami/Mudi\", \"/content/dataset/test/testmetriche/sdxl/andrea_cami1.png\")\n","\n","print(f\"SimilaritÃ  per ogni immagine di riferimento:\")\n","for path, sim in zip(result[\"ref_image_paths\"], result[\"similarities\"]):\n","    print(f\"{path}: {sim:.4f}\")\n","\n","print(f\"Media CLIP similarity: {result['mean_similarity']:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5K6eWL0RLty5","executionInfo":{"status":"ok","timestamp":1749387577574,"user_tz":-120,"elapsed":2499,"user":{"displayName":"Luigi Fienga","userId":"11281700405358534108"}},"outputId":"c00ad387-0529-4f5c-8fd1-d4e490f6a7f3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["SimilaritÃ  per ogni immagine di riferimento:\n","/content/dataset/cami/Mudi/b00.jpg: 0.6011\n","/content/dataset/cami/Mudi/b03.jpg: 0.5657\n","/content/dataset/cami/Mudi/b02.jpg: 0.5519\n","/content/dataset/cami/Mudi/b04.jpg: 0.5899\n","/content/dataset/cami/Mudi/b01.jpg: 0.5344\n","Media CLIP similarity: 0.5686\n"]}]}]}