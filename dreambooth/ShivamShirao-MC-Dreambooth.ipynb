{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["A8by-zLTep9u","bhnb0VWcdD_K"],"authorship_tag":"ABX9TyPa3SSiceSCA7Jm6ej2oPrK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Utility"],"metadata":{"id":"A8by-zLTep9u"}},{"cell_type":"code","source":["#@title Unzippare file zip\n","#@markdown Fornendo un file zip, questo script ce lo estrae in un percorso specifico che noi vogliamo.\n","\n","from google.colab import files\n","import zipfile\n","import os\n","\n","# 1. Carica file zip\n","uploaded = files.upload()\n","\n","# 2. Prende il nome del file caricato\n","zip_filename = next(iter(uploaded))\n","\n","# 3. Specifica il percorso di destinazione per l'estrazione\n","destination_path = '/content/unzipped_files' #@param {type:\"string\"}\n","\n","# 4. Crea la directory di destinazione se non esiste\n","os.makedirs(destination_path, exist_ok=True)\n","\n","# 5. Estrae il file zip nel percorso specificato\n","with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n","    zip_ref.extractall(destination_path)\n","\n","print(f\"‚úÖ File estratti in: {destination_path}\")\n"],"metadata":{"cellView":"form","id":"kQV948Nzet3Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Installazione librerie e configurazione"],"metadata":{"id":"rZYxbz5tK1_C"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"DClSkkzbKDDs"},"outputs":[],"source":["!wget -q https://github.com/ShivamShrirao/diffusers/raw/main/scripts/convert_diffusers_to_original_stable_diffusion.py\n","\n","!pip install -U -qq git+https://github.com/huggingface/diffusers.git\n","!pip install -qq accelerate tensorboard transformers ftfy gradio\n","!pip install -qq \"ipywidgets>=7,<8\"\n","!pip install -qq bitsandbytes"]},{"cell_type":"markdown","source":["Scelta di quale Training usare nell'allenamento se quello di Shivam Shrirao o il nostro, che varia per una diversa augmentation rispetto all'originale, ovvero avere immagini con: variazioni geometriche, variazioni luminose ed avere deformazioni prospettiche casuali."],"metadata":{"id":"G4U_3Jb0LW2j"}},{"cell_type":"code","source":["import os\n","\n","#@markdown:\n","scelta = \"ShivamShrirao\" #@param [\"ShivamShrirao\", \"nostro\"]\n","\n","# Controllo ed eventuale eliminazione del file esistente\n","if os.path.exists(\"train_dreambooth.py\"):\n","    os.remove(\"train_dreambooth.py\")\n","    print(\"‚úÖ File 'train_dreambooth.py' esistente rimosso.\")\n","\n","# Download del file in base alla scelta\n","if scelta == \"ShivamShrirao\":\n","    !wget -q https://github.com/ShivamShrirao/diffusers/raw/main/examples/dreambooth/train_dreambooth.py\n","    print(\"üì• Scaricato il file da ShivamShrirao.\")\n","elif scelta == \"nostro\":\n","    !wget -q https://github.com/luigifienga18/diffusers/raw/main/examples/dreambooth/train_dreambooth.py\n","    print(\"üì• Scaricato il file dal repository nostro.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6fToyoz1LnBW","executionInfo":{"status":"ok","timestamp":1749582147903,"user_tz":-120,"elapsed":312,"user":{"displayName":"Luigi Fienga","userId":"11281700405358534108"}},"outputId":"f8f0b2bb-4df3-4923-8857-4372ed1807e4"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ File 'train_dreambooth.py' esistente rimosso.\n","üì• Scaricato il file dal repository nostro.\n"]}]},{"cell_type":"code","source":["#@title Inserire il token per loggarsi su Hugginface ü§ó\n","\n","#@markdown Serve per scaricare correttamente i pesi di Stable Diffusion ed eventualmente salvarsi il modello dopo\n","!mkdir -p ~/.huggingface\n","HUGGINGFACE_TOKEN = \"\" #@param {type:\"string\"}\n","!echo -n \"{HUGGINGFACE_TOKEN}\" > ~/.huggingface/token"],"metadata":{"id":"yx6PHD5qOoGj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@markdown Inserire il path di HF per scaricare i pesi del modello\n","MODEL_NAME = \"sd-legacy/stable-diffusion-v1-5\"#@param {type:\"string\"}\n","\n","#@markdown Inserire il percorso della cartella di output\n","\n","OUTPUT_DIR = \"stable_diffusion_weights/zwx\" #@param {type:\"string\"}\n","\n","OUTPUT_DIR = \"/content/\" + OUTPUT_DIR\n","\n","print(f\"[*] Weights will be saved at {OUTPUT_DIR}\")\n","\n","!mkdir -p $OUTPUT_DIR"],"metadata":{"id":"pXf2jxtAO_pg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training"],"metadata":{"id":"c9zDUSmDWdmq"}},{"cell_type":"markdown","source":["Shivam Shirao mette a disposizione la seguente tabella per abilitare correttamente gli iperparametri necessari in base alla nostra configurazione.\n","\n","| `fp16` | `train_batch_size` | `gradient_accumulation_steps` | `gradient_checkpointing` | `use_8bit_adam` | GB VRAM usage | Speed (it/s) |\n","| ---- | ------------------ | ----------------------------- | ----------------------- | --------------- | ---------- | ------------ |\n","| fp16 | 1                  | 1                             | TRUE                    | TRUE            | 9.92       | 0.93         |\n","| no   | 1                  | 1                             | TRUE                    | TRUE            | 10.08      | 0.42         |\n","| fp16 | 2                  | 1                             | TRUE                    | TRUE            | 10.4       | 0.66         |\n","| fp16 | 1                  | 1                             | FALSE                   | TRUE            | 11.17      | 1.14         |\n","| no   | 1                  | 1                             | *FALSE*                   | TRUE            | 11.17      | 0.49         |\n","| fp16 | 1                  | 2                             | TRUE                    | TRUE            | 11.56      | 1            |\n","| fp16 | 2                  | 1                             | FALSE                   | TRUE            | 13.67      | 0.82         |\n","| fp16 | 1                  | 2                             | FALSE                   | TRUE            | 13.7       | 0.83          |\n","| fp16 | 1                  | 1                             | TRUE                    | FALSE           | 15.79      | 0.77         |\n"],"metadata":{"id":"AJwy8LS_WgHj"}},{"cell_type":"markdown","source":["Usare `--gradient_checkpointing` se si hanno a disposizione 9.92 GB VRAM.\n","\n","Rimuovere `--use_8bit_adam` per una maggiore precisione ma richiede 15.79 GB VRAM con `--gradient_checkpointing` altrimenti 17.8 GB VRAM.\n","\n","Rimuovere  `--train_text_encoder` per ridurre ulteriori consumi di memoria, al costo di poter degradare i risultati."],"metadata":{"id":"SyJdrPIMWpXI"}},{"cell_type":"code","source":["#@title Lista dei multi concetti\n","\n","concepts_list = [\n","    {\n","        \"instance_prompt\":      \"photo of camidndr person\",\n","        \"class_prompt\":         \"photo of a person\",\n","        \"instance_data_dir\":    \"/content/data/cami\",\n","        \"class_data_dir\":       \"/content/data/person\"\n","    },\n","    {\n","        \"instance_prompt\":      \"photo of anrosci person\",\n","        \"class_prompt\":         \"photo of a person\",\n","        \"instance_data_dir\":    \"/content/data/andrea\",\n","        \"class_data_dir\":       \"/content/data/person\"\n","    }\n","    ]\n","\n","# Creazione delle cartelle\n","import json\n","import os\n","for c in concepts_list:\n","    os.makedirs(c[\"instance_data_dir\"], exist_ok=True)\n","\n","with open(\"concepts_list.json\", \"w\") as f:\n","    json.dump(concepts_list, f, indent=4)"],"metadata":{"id":"RcpJGOUMWoMJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@markdown Con l'avvio di questa cella puoi fornire per ogni soggetto le immagini associate di riferiemnto\n","\n","#@markdown Se non si vuole eseguire questa cella bisogna manualmente inserire nei percorsi i file\n","\n","import os\n","from google.colab import files\n","import shutil\n","\n","for c in concepts_list:\n","    print(f\"Uploading instance images for `{c['instance_prompt']}`\")\n","    uploaded = files.upload()\n","    for filename in uploaded.keys():\n","        dst_path = os.path.join(c['instance_data_dir'], filename)\n","        shutil.move(filename, dst_path)"],"metadata":{"id":"lhrRmFwCX5FE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Inizio dell'Allenamento\n","\n","import torch\n","\n","#Pulizia della memoria della GPU per liberare spazio inutilmente occupato\n","#da eventualmente training o inferenze precedenti\n","torch.cuda.empty_cache()\n","\n","\n","!python3 train_dreambooth.py \\\n","  --pretrained_model_name_or_path=$MODEL_NAME \\\n","  --pretrained_vae_name_or_path=\"stabilityai/sd-vae-ft-mse\" \\\n","  --output_dir=$OUTPUT_DIR \\\n","  --revision=\"main\" \\\n","  --with_prior_preservation --prior_loss_weight=1.0 \\\n","  --seed=1337 \\\n","  --resolution=512 \\\n","  --train_batch_size=1 \\\n","  --train_text_encoder \\\n","  --mixed_precision=\"fp16\" \\\n","  --use_8bit_adam \\\n","  --gradient_accumulation_steps=1 \\\n","  --learning_rate=2e-6 \\\n","  --lr_scheduler=\"constant\" \\\n","  --lr_warmup_steps=0 \\\n","  --num_class_images=50 \\\n","  --sample_batch_size=4 \\\n","  --max_train_steps=1200 \\\n","  --save_interval=10000 \\\n","  --save_sample_prompt=\"photo of anrosci person\" \\\n","  --concepts_list=\"concepts_list.json\"\n","\n","# --max_train_steps deve essere sull'ordine dei 1200 per realizzare soggetti affidabili\n","# --learning_rate abbiamo notato che buoni allenamenti devono essere con valori 2e-6 o 1e-6\n","# --save_sample_prompt serve per generare degli esempi a modello allenato\n"],"metadata":{"id":"LwP03nrEZGax"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@markdown Specifica il percorso di dove sono i pesi, altrimenti lascialo vuoto, prender√† automaticamente i pesi del modello che ci servono\n","WEIGHTS_DIR = \"\" #@param {type:\"string\"}\n","if WEIGHTS_DIR == \"\":\n","    from natsort import natsorted\n","    from glob import glob\n","    import os\n","    WEIGHTS_DIR = natsorted(glob(OUTPUT_DIR + os.sep + \"*\"))[-1]\n","print(f\"[*] WEIGHTS_DIR={WEIGHTS_DIR}\")"],"metadata":{"id":"htoY15FUacuU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Inferenza"],"metadata":{"id":"-fiXXp5Aax0w"}},{"cell_type":"code","source":["#@markdown Griglia per visualizzare le immagine sintetizzate del modello.\n","import os\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","\n","weights_folder = OUTPUT_DIR\n","folders = sorted([f for f in os.listdir(weights_folder) if f != \"0\"], key=lambda x: int(x))\n","\n","row = len(folders)\n","col = len(os.listdir(os.path.join(weights_folder, folders[0], \"samples\")))\n","scale = 4\n","fig, axes = plt.subplots(row, col, figsize=(col*scale, row*scale), gridspec_kw={'hspace': 0, 'wspace': 0})\n","\n","for i, folder in enumerate(folders):\n","    folder_path = os.path.join(weights_folder, folder)\n","    image_folder = os.path.join(folder_path, \"samples\")\n","    images = [f for f in os.listdir(image_folder)]\n","    for j, image in enumerate(images):\n","        if row == 1:\n","            currAxes = axes[j]\n","        else:\n","            currAxes = axes[i, j]\n","        if i == 0:\n","            currAxes.set_title(f\"Image {j}\")\n","        if j == 0:\n","            currAxes.text(-0.1, 0.5, folder, rotation=0, va='center', ha='center', transform=currAxes.transAxes)\n","        image_path = os.path.join(image_folder, image)\n","        img = mpimg.imread(image_path)\n","        currAxes.imshow(img, cmap='gray')\n","        currAxes.axis('off')\n","\n","plt.tight_layout()\n","plt.savefig('grid.png', dpi=72)"],"metadata":{"id":"3xbictrFi-ZX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@markdown Se si sta utilizzando il seguente codice direttamente per l'inferenza bisogna specificare il percorso di dove sono memorizzati i pesi, altrimenti se si √® appena completato un allenamento usare la variabile che gi√† √® inizializzata correttamente\n","\n","#@markdown Per caricare dei pesi generici di un modello allenato in precedenza, specificare o il percorso di HF o il percorso locale di dove si trovano nella macchina.\n","\n","#@markdown Scegli la sorgente del modello da caricare:\n","sorgente_modello = \"Usa WEIGHTS_DIR\" #@param [\"Usa WEIGHTS_DIR\", \"Inserisci percorso manuale\"]\n","\n","if sorgente_modello == \"Usa WEIGHTS_DIR\":\n","    model_path = WEIGHTS_DIR\n","else:\n","    #@markdown Inserisci il percorso completo del modello da prelevare:\n","    model_path = \"/content/mio_modello\" #@param {type:\"string\"}\n","\n","import torch\n","from torch import autocast\n","from diffusers import StableDiffusionPipeline, DDIMScheduler\n","from IPython.display import display\n","\n","pipe = StableDiffusionPipeline.from_pretrained(\n","    model_path,\n","    safety_checker=None,\n","    torch_dtype=torch.float16\n",").to(\"cuda\")\n","\n","pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n","g_cuda = None\n"],"metadata":{"id":"4vKpVQbIa0hr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@markdown Se si preferisce √® possibile fornire un seed diverso da quello inizializzato\n","g_cuda = torch.Generator(device='cuda')\n","seed = 52362 #@param {type:\"number\"}\n","g_cuda.manual_seed(seed)"],"metadata":{"id":"i3rNgLIIcunM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Esecuzione\n","\n","prompt = \"photo of luifienga person kissing anrosci person\" #@param {type:\"string\"}\n","negative_prompt = \"blurry, low quality, deformed, distorted, bad anatomy, extra limbs, out of frame\" #@param {type:\"string\"}\n","num_samples = 4 #@param {type:\"number\"}\n","guidance_scale = 7 #@param {type:\"number\"}\n","num_inference_steps = 80 #@param {type:\"number\"}\n","height = 512 #@param {type:\"number\"}\n","width = 512 #@param {type:\"number\"}\n","\n","with autocast(\"cuda\"), torch.inference_mode():\n","    images = pipe(\n","        prompt,\n","        height=height,\n","        width=width,\n","        negative_prompt=negative_prompt,\n","        num_images_per_prompt=num_samples,\n","        num_inference_steps=num_inference_steps,\n","        guidance_scale=guidance_scale,\n","        generator=g_cuda\n","    ).images\n","\n","for img in images:\n","    display(img)"],"metadata":{"cellView":"form","id":"wCdkUhyUc48H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Upload su HUGGING FACE"],"metadata":{"id":"bhnb0VWcdD_K"}},{"cell_type":"code","source":["#@markdown Inserisci il tuo Hugging Face Token\n","hf_token = \"\"  #@param {type:\"string\"}\n","\n","#@markdown Inserisci il percorso locale del modello addestrato\n","local_model_path = \"/content/stable_diffusion_weights/zwx/1200\"  #@param {type:\"string\"}\n","\n","#@markdown Inserisci il nome del repository Hugging Face da creare o usare\n","repo_name = \"Dreambooth-CamiAndrea-ShivamShrirao\"  #@param {type:\"string\"}\n","\n","#@markdown (Opzionale) Inserisci il nome dell'organizzazione se vuoi pubblicarlo l√¨\n","organization = \"\"  #@param {type:\"string\"}\n","\n","from diffusers import DiffusionPipeline\n","from huggingface_hub import login, create_repo\n","import os\n","\n","# Step 1: Login\n","login(token=hf_token)\n","\n","# Step 2: Carica il pipeline addestrato\n","pipeline = DiffusionPipeline.from_pretrained(local_model_path)\n","\n","# Step 3: Prepara il nome del repository\n","repo_id = f\"{organization.strip() + '/' if organization.strip() else ''}{repo_name.strip()}\"\n","\n","# Step 4: Scrivi README.md\n","readme_path = os.path.join(local_model_path, \"README.md\")\n","\n","#@markdown Per modificare la descrizione aprire la tendina di mostra codice e cambiarla\n","description = \"\"\"\n","# Dreambooth-CamiAndrea-ShivamShrirao\n","\n","Questo modello √® stato addestrato usando DreamBooth su pi√π concetti.\n","\n","## Dettagli\n","\n","- Basato su Stable Diffusion\n","- Realizza Foto di Camilla e Andrea\n","- Token Andrea: `anrosc person`\n","- Token Camilla: `camdand person`\n","- Addestrato con pi√π soggetti per supportare la generazione multi-concept\n","- Usare i token personalizzati indicati nella documentazione\n","- Modello addestrato sul primo dataset di Dreambooth presente nel Drive\n","\n","Per ogni dettaglio tecnico, controlla i file di configurazione.\n","\"\"\"\n","\n","with open(readme_path, \"w\") as f:\n","    f.write(description)\n","\n","# Step 5: Crea il repo (se non esiste gi√†)\n","create_repo(repo_id, exist_ok=True)\n","\n","# Step 6: Pubblica il modello su Hugging Face Hub\n","pipeline.push_to_hub(repo_id)\n","\n","print(f\"‚úÖ Modello caricato su: https://huggingface.co/{repo_id}\")\n"],"metadata":{"id":"E6XC36DTdHlD"},"execution_count":null,"outputs":[]}]}